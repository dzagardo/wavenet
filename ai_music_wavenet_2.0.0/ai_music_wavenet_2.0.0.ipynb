{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4464874-d14a-4076-ae9d-d2c08f515933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common shape: (128, 5157)\n",
      "Array at index 199 has shape: (128, 937)\n",
      "Shapes after preprocessing:\n",
      "X_train: (200, 128, 5157)\n",
      "y_train: (200, 480000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "### Convert training data to mel spectrograms ###\n",
    "\n",
    "def wav_to_mel_spectrogram(y, sr=48000, hop_length=512, n_fft=2048, n_mels=128):\n",
    "    # Compute the mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to log scale\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "file_path = './trainingdata_v2.wav'\n",
    "sr = 48000\n",
    "\n",
    "# Load the lengthy audio file in chunks\n",
    "frame_length = sr * 10  # 10 seconds per frame\n",
    "hop_size = sr * 5       # 5 seconds overlap\n",
    "\n",
    "# Store mel spectrograms and corresponding audio slices\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "stream = librosa.stream(file_path, block_length=10, frame_length=frame_length, hop_length=hop_size, mono=True)\n",
    "\n",
    "for y in stream:\n",
    "    mel_spec = wav_to_mel_spectrogram(y)\n",
    "    \n",
    "    # Using the entire frame for audio data\n",
    "    X_train.append(mel_spec)\n",
    "    y_train.append(y[:frame_length])\n",
    "\n",
    "# Padding the last array in X_train\n",
    "expected_columns = (frame_length // 512)  # 512 is the hop_length\n",
    "# Check the shape of the last element in X_train\n",
    "if X_train[-1].shape[1] != expected_columns:\n",
    "    padded_shape = (128, max(expected_columns, X_train[-1].shape[1]))\n",
    "    padded = np.zeros(padded_shape)\n",
    "    padded[:, :X_train[-1].shape[1]] = X_train[-1]\n",
    "    X_train[-1] = padded[:, :expected_columns]\n",
    "\n",
    "# Padding the last array in y_train\n",
    "if y_train[-1].shape[0] != frame_length:\n",
    "    padded = np.zeros(frame_length)\n",
    "    padded[:y_train[-1].shape[0]] = y_train[-1]\n",
    "    y_train[-1] = padded\n",
    "\n",
    "# Step 1: Identify Inconsistent Shapes\n",
    "\n",
    "# Find the most common shape\n",
    "shapes = [x.shape for x in X_train]\n",
    "most_common_shape = max(set(shapes), key=shapes.count)\n",
    "\n",
    "print(\"Most common shape:\", most_common_shape)\n",
    "\n",
    "# Print the shapes that are different from the most common shape\n",
    "for i, shape in enumerate(shapes):\n",
    "    if shape != most_common_shape:\n",
    "        print(f\"Array at index {i} has shape: {shape}\")\n",
    "\n",
    "# Step 2: Correct the Shapes by Padding\n",
    "\n",
    "# Define the expected number of columns based on the most common shape\n",
    "expected_columns = most_common_shape[1]\n",
    "\n",
    "# Pad the last mel spectrogram in X_train\n",
    "if X_train[-1].shape[1] != expected_columns:\n",
    "    padded = np.zeros((128, expected_columns))\n",
    "    padded[:, :X_train[-1].shape[1]] = X_train[-1]\n",
    "    X_train[-1] = padded\n",
    "\n",
    "# Once again, try converting the list to a numpy array\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# Sanity check\n",
    "assert len(X_train) == len(y_train), \"Mismatched lengths between X_train and y_train!\"\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(\"Shapes after preprocessing:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52a5fb7-e3d8-46cc-8328-2f9c620bad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 13:10:26.385609: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Add, Activation, Multiply, Flatten, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2105206b-9e0a-4fab-b3af-0b534e29e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked data shape: (15800, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "def chunk_data(data, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunk 3D data along the last axis.\n",
    "    \n",
    "    :param data: 3D numpy array with shape (num_samples, num_features, time_steps)\n",
    "    :param chunk_size: Size of each chunk along the time_steps axis\n",
    "    :param overlap: Overlap between consecutive chunks\n",
    "    :return: Chunked data as a 3D numpy array\n",
    "    \"\"\"\n",
    "    num_samples, num_features, time_steps = data.shape\n",
    "    step_size = chunk_size - overlap\n",
    "    num_chunks = (time_steps - overlap) // step_size\n",
    "    \n",
    "    # Prepare an array to hold the chunked data\n",
    "    chunks = np.zeros((num_samples * num_chunks, num_features, chunk_size))\n",
    "    \n",
    "    chunk_idx = 0\n",
    "    for i in range(num_samples):\n",
    "        for j in range(0, time_steps - chunk_size + 1, step_size):\n",
    "            chunks[chunk_idx] = data[i, :, j:j+chunk_size]\n",
    "            chunk_idx += 1\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Define parameters\n",
    "chunk_size = 128  # This can be adjusted based on the model's receptive field and other considerations.\n",
    "overlap = 64  # Half the chunk size for a 50% overlap.\n",
    "\n",
    "# Chunk the mel spectrogram data\n",
    "X_chunked = chunk_data(X_train, chunk_size, overlap)\n",
    "\n",
    "print(\"Chunked data shape:\", X_chunked.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ab0b37-c6c2-4e86-9e14-dc0220eb1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked y data shape: (1499800, 128)\n"
     ]
    }
   ],
   "source": [
    "def chunk_data_2d(data, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunk 2D data along the last axis.\n",
    "    \n",
    "    :param data: 2D numpy array with shape (num_samples, time_steps)\n",
    "    :param chunk_size: Size of each chunk along the time_steps axis\n",
    "    :param overlap: Overlap between consecutive chunks\n",
    "    :return: Chunked data as a 2D numpy array\n",
    "    \"\"\"\n",
    "    num_samples, time_steps = data.shape\n",
    "    step_size = chunk_size - overlap\n",
    "    num_chunks = (time_steps - overlap) // step_size\n",
    "    \n",
    "    # Prepare an array to hold the chunked data\n",
    "    chunks = np.zeros((num_samples * num_chunks, chunk_size))\n",
    "    \n",
    "    chunk_idx = 0\n",
    "    for i in range(num_samples):\n",
    "        for j in range(0, time_steps - chunk_size + 1, step_size):\n",
    "            chunks[chunk_idx] = data[i, j:j+chunk_size]\n",
    "            chunk_idx += 1\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Chunk the target data\n",
    "y_chunked = chunk_data_2d(y_train, chunk_size, overlap)\n",
    "\n",
    "print(\"Chunked y data shape:\", y_chunked.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c93f82-d40b-4ab2-9617-8fa28c06c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the generated mel spectrogram:\", generated_mel_spectrogram.shape)\n",
    "generated_mel_spectrogram = np.array(generated_mel_spectrogram)\n",
    "\n",
    "# Step 2: Convert each chunk of the mel spectrogram sequence back to waveform\n",
    "def mel_to_audio(mel_spectrogram, sr=48000, hop_length=256, n_iter=50):\n",
    "    # Inverse mel scale\n",
    "    mel_to_linear = librosa.feature.inverse.mel_to_stft(mel_spectrogram, sr=sr)\n",
    "    # Spectrogram to waveform using Griffin-Lim\n",
    "    waveform = librosa.griffinlim(mel_to_linear, hop_length=hop_length, n_iter=n_iter)\n",
    "    return waveform\n",
    "\n",
    "audio_outputs = []\n",
    "for idx, chunk in enumerate(generated_mel_spectrogram):\n",
    "    audio_outputs.append(mel_to_audio(chunk, sr=48000, hop_length=256))\n",
    "    if (idx + 1) % 100 == 0:  # Print progress every 100 chunks\n",
    "        print(f\"Processed {idx + 1} chunks out of {len(generated_mel_spectrogram)}\")\n",
    "\n",
    "audio_output = np.concatenate(audio_outputs)\n",
    "print(\"All chunks processed and concatenated!\")\n",
    "\n",
    "# Step 3: Save the waveform as a .wav file\n",
    "sf.write('generated_output.wav', audio_output, 48000)\n",
    "print(\"'generated_output.wav' saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b375f7-1ff5-400a-9006-036b0478264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46869/46869 [==============================] - 2046s 44ms/step - loss: 0.0173 - mae: 0.0829 - lr: 0.0010\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidzagardo/anaconda3/envs/wavenet/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46869/46869 [==============================] - 2064s 44ms/step - loss: 0.0172 - mae: 0.0822 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "46869/46869 [==============================] - 2004s 43ms/step - loss: 0.0171 - mae: 0.0821 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "46869/46869 [==============================] - 1969s 42ms/step - loss: 0.0171 - mae: 0.0820 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "46869/46869 [==============================] - 2043s 44ms/step - loss: 0.0171 - mae: 0.0820 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "46869/46869 [==============================] - 2111s 45ms/step - loss: 0.0171 - mae: 0.0820 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "46869/46869 [==============================] - ETA: 0s - loss: 0.0171 - mae: 0.0820\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "46869/46869 [==============================] - 1998s 43ms/step - loss: 0.0171 - mae: 0.0820 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "46869/46869 [==============================] - 2244s 48ms/step - loss: 0.0171 - mae: 0.0819 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "46869/46869 [==============================] - 2239s 48ms/step - loss: 0.0171 - mae: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "46869/46869 [==============================] - 2444s 52ms/step - loss: 0.0171 - mae: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "46869/46869 [==============================] - 2522s 54ms/step - loss: 0.0171 - mae: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "46244/46869 [============================>.] - ETA: 33s - loss: 0.0175 - mae: 0.0827"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Multiply, Add, Activation, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Definition\n",
    "def generate_model(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # Gate mechanism 1\n",
    "    conv_a = Conv1D(40, 3, padding='same', activation='sigmoid')(inp)\n",
    "    conv_b = Conv1D(40, 3, padding='same', activation='tanh')(inp)\n",
    "    mult_1 = Multiply()([conv_a, conv_b])\n",
    "    skip_1 = Conv1D(40, 1, padding='same')(mult_1)\n",
    "    skip_1_connect = Conv1D(40, 1, padding='same')(inp)\n",
    "    res_1 = Add()([skip_1, skip_1_connect])\n",
    "\n",
    "    # Repeated blocks\n",
    "    skips = []\n",
    "    for _ in range(5):\n",
    "        conv_a = Conv1D(40, 3, padding='same', activation='sigmoid')(res_1)\n",
    "        conv_b = Conv1D(40, 3, padding='same', activation='tanh')(res_1)\n",
    "        mult = Multiply()([conv_a, conv_b])\n",
    "        skip = Conv1D(40, 1, padding='same')(mult)\n",
    "        skips.append(skip)\n",
    "        skip_connect = Conv1D(40, 1, padding='same')(res_1)\n",
    "        res_1 = Add()([skip, skip_connect])\n",
    "\n",
    "    sum_skips = Add()(skips)\n",
    "    act = Activation('relu')(sum_skips)\n",
    "    out_conv1 = Conv1D(40, 1, padding='same', activation='relu')(act)\n",
    "    \n",
    "    # Adjusting the output to match target shape\n",
    "    out_conv2 = Conv1D(1, 1, padding='same')(out_conv1)  # One filter for audio wave sequence\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out_conv2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming y_chunked is already prepared, with shape (1499800, 128)\n",
    "y_input = y_chunked[:-1]   # All except the last one\n",
    "y_target = y_chunked[1:]   # All except the first one\n",
    "\n",
    "# Reshape the data to fit the model's expected input shape\n",
    "y_input = y_input.reshape(-1, 128, 1)\n",
    "y_target = y_target.reshape(-1, 128, 1)\n",
    "\n",
    "# Create TensorFlow datasets for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((y_input, y_target))\n",
    "train_dataset = train_dataset.batch(32).shuffle(buffer_size=1024).cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Wavenet-like architecture\n",
    "model = generate_model(input_shape=(128, 1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('best_weights.h5', save_best_only=True, monitor='loss', mode='min'),\n",
    "    keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=100, callbacks=callbacks)\n",
    "\n",
    "# Save the entire model to a HDF5 file.\n",
    "model.save('ai_music_wavenet_wavfile_1_0_0.h5')\n",
    "\n",
    "loaded_model = keras.models.load_model('ai_music_wavenet_wavfile_1_0_0.h5')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')   # changed 'mean_absolute_error' to 'mae'\n",
    "plt.legend()\n",
    "plt.title('Metric Evolution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cdaf2-57b4-47b4-b1ba-f8987988cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Assuming the model is already trained and loaded\n",
    "# loaded_model = keras.models.load_model('ai_music_wavenet_wavfile_1_0_0.h5')\n",
    "\n",
    "def generate_audio_sequence(model, seed_sequence, target_length):\n",
    "    \"\"\"\n",
    "    Generate a sequence of audio samples.\n",
    "    \n",
    "    :param model: The trained WaveNet model\n",
    "    :param seed_sequence: A seed audio sequence to start the generation\n",
    "    :param target_length: The target number of samples to generate\n",
    "    :return: The generated audio sequence\n",
    "    \"\"\"\n",
    "    generated_audio = seed_sequence\n",
    "    while generated_audio.shape[0] < target_length:\n",
    "        # Predict the next sample\n",
    "        next_sample = model.predict(generated_audio[-128:].reshape(1, 128, 1))\n",
    "        # Append the sample\n",
    "        generated_audio = np.append(generated_audio, next_sample[0, -1, 0])  # Append the last value\n",
    "\n",
    "        # Print progress\n",
    "        if generated_audio.shape[0] % 48000 == 0:\n",
    "            print(f\"Generated {generated_audio.shape[0] / 48000} seconds of audio...\")\n",
    "\n",
    "    # If we have exceeded the target length, trim the sequence\n",
    "    if generated_audio.shape[0] > target_length:\n",
    "        generated_audio = generated_audio[:target_length]\n",
    "    return generated_audio\n",
    "\n",
    "# Generate 30 seconds of audio\n",
    "seed_sequence = y_input[0]  # Use the first sequence from y_chunked as a seed\n",
    "generated_sequence = generate_audio_sequence(loaded_model, seed_sequence, 720000)\n",
    "\n",
    "# Save the generated audio sequence to a WAV file\n",
    "sf.write('generated_music.wav', generated_sequence, 48000)\n",
    "print(\"Audio generation complete and saved to 'generated_music.wav'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
